{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment"
      ],
      "metadata": {
        "id": "ghuOdFK_i258"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "Answer ->\n",
        "\n",
        "Ensemble Learning in machine learning is a technique where multiple models (called \"base learners\" or \"weak learners\") are trained and combined to solve the same problem, with the goal of achieving better performance than any single model alone.\n",
        "\n",
        "**Key Idea Behind Ensemble Learning :**\n",
        "\n",
        "The main idea is that a group of weak models working together can outperform a single strong model — similar to the saying “wisdom of the crowd.”\n",
        "\n",
        "Different models may make different errors, and by combining their predictions, these errors can often cancel each other out, resulting in higher accuracy, better generalization, and improved robustness."
      ],
      "metadata": {
        "id": "ehE_nmFJi28f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "**Bagging :**\n",
        "- Models are built independently in parallel\n",
        "- Goal- Reduce variance\n",
        "- Data smapling uses random subsets of data with replacement\n",
        "- Combination method uses simple averaging or voting\n",
        "- Common algorithm Random Forest, Bagged Trees\n",
        "\n",
        "**Boosting :**\n",
        "- Models are built sequentially, each learning from the previous one’s errors\n",
        "- Goal- Reduce bias\n",
        "- Data sampling uses the entire dataset, adjusting weights of samples\n",
        "- Combination method uses weighted averaging of models\n",
        "- Common algorithm AdaBoost, Gradient Boosting, XGBoost\n"
      ],
      "metadata": {
        "id": "Pme_aVN9i2-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "**Bootstrap sampling :**\n",
        "\n",
        "Bootstrap sampling is a statistical technique that involves randomly selecting samples from a dataset with replacement to create multiple new datasets (called bootstrap samples).\n",
        "\n",
        "Each bootstrap sample is of the same size as the original dataset.\n",
        "\n",
        "Because sampling is done with replacement, some data points may appear multiple times, while others may not appear at all in a given sample.\n",
        "\n",
        "Example:\n",
        "If you have a dataset with 10 records, a bootstrap sample might look like:\n",
        "\n",
        "[2, 5, 1, 7, 5, 9, 3, 1, 8, 4]\n",
        "\n",
        "Here, some elements (like 5 and 1) appear twice, and some (like 6 or 10) might not appear.\n",
        "\n",
        "**Role of Bootstrap Sampling in Bagging :**\n",
        "\n",
        "Bagging (Bootstrap Aggregating) uses bootstrap sampling as its foundation.\n",
        "\n",
        "Process:\n",
        "\n",
        "1. From the original dataset, multiple bootstrap samples are generated.\n",
        "\n",
        "2. A separate model (e.g., decision tree) is trained on each bootstrap sample independently.\n",
        "\n",
        "3. The predictions of all models are combined (by averaging for regression or majority voting for classification).\n",
        "\n",
        "This aggregation helps reduce the variance of the final model."
      ],
      "metadata": {
        "id": "Tl3Jv4c4i3A9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "Answer ->\n",
        "\n",
        "**Out-of-Bag :**\n",
        "\n",
        "When performing bootstrap sampling (sampling with replacement) in ensemble methods like Bagging or Random Forest, not all data points from the original dataset are selected in each bootstrap sample.\n",
        "\n",
        "On average, each bootstrap sample contains about 63–67% of the original data.\n",
        "\n",
        "The remaining ~33% of the data, which are not selected in that particular sample, are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "OOB samples are the data points left out during the creation of a bootstrap sample.\n",
        "\n",
        "**Role of OOB Samples in Model Evaluation**\n",
        "\n",
        "The OOB samples serve as a built-in validation set for evaluating the model’s performance without needing a separate test set.\n",
        "\n",
        "How it works (in Random Forest or Bagging):\n",
        "\n",
        "1. For each tree (or base model), the OOB samples (those not used for training that tree) are passed through the model to get predictions.\n",
        "\n",
        "2. For every observation in the dataset, you can collect predictions from all the trees for which that observation was OOB.\n",
        "\n",
        "3. Compare the aggregated OOB predictions to the true labels to estimate the OOB error or OOB score."
      ],
      "metadata": {
        "id": "RnkjUZD4i3DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "Answer ->\n",
        "\n",
        "**Decision Tree :**\n",
        "- Based on impurity reduction in one tree\n",
        "- Unstable — sensitive to data changes\n",
        "- Bias can favor features with many levels or splits\n",
        "- Easier to interpret (simple structure)\n",
        "- Reflects importance in one specific tree\n",
        "\n",
        "**Random Forest :**\n",
        "- Average of impurity reductions across all trees\n",
        "- Stable — averages across multiple trees\n",
        "- Reduces this bias due to feature randomness\n",
        "- Harder to interpret (aggregated result)\n",
        "- Reflects overall importance across the ensemble"
      ],
      "metadata": {
        "id": "ovYJKOFWi3GT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.6.  Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "\n",
        "    sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "M9sJOiHRs1QN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# Step 1: Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "\n",
        "# Convert to DataFrame for easier handling\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Step 2: Train a Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Step 3: Get feature importance scores\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame of feature names and their importance scores\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': data.feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Step 4: Sort features by importance in descending order\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Step 5: Print the top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCGtW2-BtMvK",
        "outputId": "50503940-7b95-4c00-a667-f2e0f894901b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.7. Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "gIJwaAiMs1ME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train a single Decision Tree classifier\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# Step 4: Train a Bagging Classifier using Decision Trees as base estimators\n",
        "bagging = BaggingClassifier(\n",
        "    base_estimator=DecisionTreeClassifier(),\n",
        "    n_estimators=50,       # number of trees\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# Step 5: Print and compare the accuracies\n",
        "print(\"Accuracy of Single Decision Tree:\", round(dt_accuracy, 3))\n",
        "print(\"Accuracy of Bagging Classifier (with Decision Trees):\", round(bagging_accuracy, 3))\n"
      ],
      "metadata": {
        "id": "tUAhDy1Et3zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.8. : Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "F1JOU35ms1IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define the base Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Step 4: Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [3, 5, 7, None]\n",
        "}\n",
        "\n",
        "# Step 5: Use GridSearchCV to find the best combination of parameters\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1            # Use all available cores for faster computation\n",
        ")\n",
        "\n",
        "# Step 6: Fit GridSearchCV on training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Get the best parameters and model\n",
        "best_params = grid_search.best_params_\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Step 8: Evaluate the final model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 9: Print results\n",
        "print(\"Best Parameters Found:\", best_params)\n",
        "print(\"Final Model Test Accuracy:\", round(accuracy, 3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCSbGfAOuDQx",
        "outputId": "ad4ea829-310c-41b4-e0fb-c25dd3c6fda3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters Found: {'max_depth': 3, 'n_estimators': 150}\n",
            "Final Model Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.9. Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "FRYD1jHQs1Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Answer ->>\n",
        "\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 1: Load California Housing dataset\n",
        "california = fetch_california_housing()\n",
        "X, y = california.data, california.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train a Bagging Regressor (with Decision Trees)\n",
        "bagging = BaggingRegressor(\n",
        "    base_estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging.fit(X_train, y_train)\n",
        "y_pred_bag = bagging.predict(X_test)\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "\n",
        "# Step 4: Train a Random Forest Regressor\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42\n",
        ")\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred_rf = rf.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Step 5: Print and compare MSEs\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", round(mse_bag, 3))\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", round(mse_rf, 3))\n"
      ],
      "metadata": {
        "id": "esqDHIyWueFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q.10. You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer ->\n",
        "\n",
        "1. Data Preparation : Clean, impute missing values, encode categorical features, scale numeric features if needed.\n",
        "\n",
        "2. Model Choice : Decide Bagging (Random Forest) vs Boosting (XGBoost/Gradient Boosting).\n",
        "\n",
        "3. Base Learner Selection : Shallow Decision Trees for boosting; full trees for bagging.\n",
        "\n",
        "4. Hyperparameter Tuning & Overfitting Control : Cross-validation, regularization, max depth, learning rate.\n",
        "\n",
        "5. Model Evaluation : Stratified k-fold CV, ROC-AUC, Precision/Recall, F1-score.\n",
        "\n",
        "6. Deployment & Monitoring : Use ensemble predictions to guide lending decisions; monitor model drift over time."
      ],
      "metadata": {
        "id": "b5OkNMods1E2"
      }
    }
  ]
}